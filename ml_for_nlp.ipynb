{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUJYJSuTeJqyvKN23K+xGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SufyAD/AI-ML/blob/nlp/ml_for_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical examples of NLP are:\n",
        "-"
      ],
      "metadata": {
        "id": "sF7qRgC1_q8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is `NLTK` and Why Are We Using It?\n",
        "\n",
        "`NLTK` (Natural Language Toolkit) is a powerful **Python library** designed for working with human language data (text). It offers user-friendly interfaces to over 50 corpora and lexical resources, including WordNet, and provides a comprehensive suite of tools for text processing tasks such as:\n",
        "\n",
        "* Tokenization\n",
        "\n",
        "* Stemming\n",
        "\n",
        "* Tagging\n",
        "\n",
        "* Parsing\n",
        "\n",
        "* Classification\n",
        "\n",
        "* Semantic reasoning\n",
        "\n",
        "We use `NLTK` to perform foundational **NLP operations efficiently** and to access rich linguistic resources for building robust language-processing application such as NLP bots, and real-world chatbots"
      ],
      "metadata": {
        "id": "Oe6u8vFFBoIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Stemming techniques and their drawbacks"
      ],
      "metadata": {
        "id": "Fnj1f-nQiaQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hurXDjgc9-Y",
        "outputId": "221ec2f6-2019-41a6-c6ca-6c5636502897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg1rS3DSArg1"
      },
      "outputs": [],
      "source": [
        "# Stemming technique\n",
        "words = [\n",
        "    'preprocessing', 'unbelievably', 'counterintuitive', 'misunderstanding',\n",
        "    'reimplementation', 'overcompensating', 'underachievement', 'disenfranchisement',\n",
        "    'miscommunication', 'internationalization', 'antidisestablishmentarianism',\n",
        "    'bioengineering', 'microencapsulation', 'hyperresponsiveness', 'reconfiguration',\n",
        "    'suboptimization', 'transcontinental', 'overspecialization', 'deconstructionist'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PorterStemmer\n",
        "A word stemmer based on the Porter stemming algorithm.\n",
        "\n",
        "> Porter, M. \"An algorithm for suffix stripping.\"\n"
      ],
      "metadata": {
        "id": "Nw9i-NBBg3eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "for word in words:\n",
        "  print ('------>')\n",
        "  print(stemmer.stem(word)) # stemming words\n",
        "# Disadvantages with Stemming\n",
        "# it will deform the words and update such words that donot even exists"
      ],
      "metadata": {
        "id": "Jik5jEg5c7im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "htDhZgzDhAZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegexpStemmer\n",
        "A word stemmer based on the Porter stemming algorithm.\n",
        "\n",
        "> Porter, M. \"An algorithm for suffix stripping.\"\n",
        "\n",
        "\n",
        "```\n",
        "RegexpStemmer(regex, min)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ddYu0r2NhHI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer = RegexpStemmer('ing|able|ion|ent|ness', min=4) # set variables\n",
        "for word in words:\n",
        "  print(f\"{word} -> {reg_stemmer.stem(word)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWZKiD3ZfTm6",
        "outputId": "edcb95c9-a4a1-4c07-f5e4-7067d981c04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing -> preprocess\n",
            "unbelievably -> unbelievably\n",
            "counterintuitive -> counterintuitive\n",
            "misunderstanding -> misunderstand\n",
            "reimplementation -> reimplemat\n",
            "overcompensating -> overcompensat\n",
            "underachievement -> underachievem\n",
            "disenfranchisement -> disenfranchisem\n",
            "miscommunication -> miscommunicat\n",
            "internationalization -> internatalizat\n",
            "antidisestablishmentarianism -> antidisestablishmarianism\n",
            "bioengineering -> bioengineer\n",
            "microencapsulation -> microencapsulat\n",
            "hyperresponsiveness -> hyperresponsive\n",
            "reconfiguration -> reconfigurat\n",
            "suboptimization -> suboptimizat\n",
            "transcontinental -> transcontinal\n",
            "overspecialization -> overspecializat\n",
            "deconstructionist -> deconstructist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Lemmetization > Stemming"
      ],
      "metadata": {
        "id": "4h7OCZDsii6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwYgVIl8mQj5",
        "outputId": "f1bd01d5-f959-4a3b-c292-27149f8e0315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer # best version to convert words to root words\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmetizer = WordNetLemmatizer()\n",
        "\n",
        "# get pos of the words in the list implied by the pos_tag nltk fx\n",
        "def get_pos(treebank_tag):\n",
        "  if treebank_tag.startswith('J'):  # Adjective\n",
        "    return wordnet.ADJ\n",
        "  elif treebank_tag.startswith('V'): # Verb\n",
        "    return wordnet.VERB\n",
        "  elif treebank_tag.startswith('N'): # Noun\n",
        "    return wordnet.NOUN\n",
        "  elif treebank_tag.startswith('R'): # Adverb\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN\n",
        "\n",
        "tagged = pos_tag(words) # this will add tag against each word of the list/para\n",
        "for word, tag in tagged:\n",
        "  print(f\"{word} -> {lemmetizer.lemmatize(word, get_pos(tag))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASWfI83UgQBw",
        "outputId": "056a3573-fd7b-47cf-9749-1e1e06a51df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing -> preprocessing\n",
            "unbelievably -> unbelievably\n",
            "counterintuitive -> counterintuitive\n",
            "misunderstanding -> misunderstand\n",
            "reimplementation -> reimplementation\n",
            "overcompensating -> overcompensate\n",
            "underachievement -> underachievement\n",
            "disenfranchisement -> disenfranchisement\n",
            "miscommunication -> miscommunication\n",
            "internationalization -> internationalization\n",
            "antidisestablishmentarianism -> antidisestablishmentarianism\n",
            "bioengineering -> bioengineering\n",
            "microencapsulation -> microencapsulation\n",
            "hyperresponsiveness -> hyperresponsiveness\n",
            "reconfiguration -> reconfiguration\n",
            "suboptimization -> suboptimization\n",
            "transcontinental -> transcontinental\n",
            "overspecialization -> overspecialization\n",
            "deconstructionist -> deconstructionist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing stopwords and do Lemmetization\n",
        "- we will use nltk.stopwords (from the stopwords library of nltk)\n",
        "- do lemmetization"
      ],
      "metadata": {
        "id": "VaoPSxf_oNUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "jOjIjtKQuM5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "Dr. Abdus Salam, in his Nobel Prize acceptance speech, emphasized the vital role of science in the progress of developing nations.\n",
        "He spoke about the unity of scientific thought across cultures and highlighted the contributions of Muslim scientists throughout history.\n",
        "With humility, he dedicated his award to the poor of the Third World, whose struggles inspired his pursuit of knowledge.\n",
        "His words reflected a deep belief in the power of education and global collaboration for a better future.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UEB5rjiDtFVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import wordnet\n",
        "# Step 1 : Tokenize\n",
        "sentence = []\n",
        "sentence = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Step 2: Remove step words and do lemmetization\n",
        "lemmetizer = WordNetLemmatizer()\n",
        "for sentence in sentence:\n",
        "  words = word_tokenize(sentence)\n",
        "  words = [lemmetizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentence = ' '.join(words)\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC8BS2NJoZED",
        "outputId": "a8baf021-9921-4fa8-9421-c3a0382b697f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Abdus Salam , Nobel Prize acceptance speech , emphasized vital role science progress developing nation .\n",
            "He spoke unity scientific thought across culture highlighted contribution Muslim scientist throughout history .\n",
            "With humility , dedicated award poor Third World , whose struggle inspired pursuit knowledge .\n",
            "His word reflected deep belief power education global collaboration better future .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding **Part Of Speech** Tagging in this project of removing the stopwords and Lemmetization\n",
        "1. Do sentence tokenization\n",
        "2. Remove stopwords and do word_tokenization\n",
        "3. Add pos_tag to each words after removign stopwords\n",
        "4. Do lemmatization w.r.t the pos_tag for each words\n",
        "5. .join the lemmatized words to form a pre-processed sentence again"
      ],
      "metadata": {
        "id": "WPeSPnuJvfj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Step 1 : Tokenize\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Step 2: Remove the stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for sent in sentences:\n",
        "    words = nltk.word_tokenize(sent)\n",
        "    words = [word for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "\n",
        "    # Step 3: POS tagging\n",
        "    pos_tagged = nltk.pos_tag(words)\n",
        "\n",
        "    # Remove stopping words and do lemmetization with `Parts Of Speech`\n",
        "    l_words = []\n",
        "    for word, tag in pos_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            l_words.append(lemmatizer.lemmatize(word, wordnet.ADJ))\n",
        "        elif tag.startswith('V'):\n",
        "            l_words.append(lemmatizer.lemmatize(word, wordnet.VERB))\n",
        "        elif tag.startswith('R'):\n",
        "            l_words.append(lemmatizer.lemmatize(word, wordnet.ADV))\n",
        "        else:\n",
        "            l_words.append(lemmatizer.lemmatize(word)) # default pos = NOUN\n",
        "    sentence = ' '.join(l_words)\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29YoAG8vnTQ",
        "outputId": "525fe9e7-5d2b-4a39-f518-371862e9c986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Abdus Salam , Nobel Prize acceptance speech , emphasize vital role science progress develop nation .\n",
            "spoke unity scientific think across culture highlight contribution Muslim scientist throughout history .\n",
            "humility , dedicate award poor Third World , whose struggle inspire pursuit knowledge .\n",
            "word reflect deep belief power education global collaboration well future .\n"
          ]
        }
      ]
    }
  ]
}