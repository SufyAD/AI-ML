{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Here's a more descriptive and professional version of your README snippet:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ Learning Objectives\n",
    "\n",
    "By the end of this project, you will have hands-on experience with the following concepts critical to building an agentic chatbot:\n",
    "\n",
    "- **Data Ingestion Pipeline**  \n",
    "  Learn how to efficiently feed custom data into a chatbot agent to enable context-aware responses.\n",
    "\n",
    "- **Data Preprocessing and Splitting**  \n",
    "  Understand how to structure and split data for optimal processing and downstream performance.\n",
    "\n",
    "- **Text Embedding Techniques**  \n",
    "  Apply embedding strategies to convert textual data into vector representations suitable for semantic search and retrieval.\n",
    "\n",
    "- **Knowledge Integration in Chatbots**  \n",
    "  Discover how to enhance chatbot capabilities using embedded knowledge, enabling dynamic and intelligent interactions.\n",
    "\n",
    "- **Tool-Based Agent Frameworks**  \n",
    "  Gain familiarity with agentic patterns and how to incorporate tools that allow your chatbot to reason and act based on user inputs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Testing with langchain Textloaders, and check how it works\n",
    "from langchain_community.document_loaders import TextLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://docs.astral.sh/uv/guides/integration/jupyter/#creating-a-kernel\",\n",
    "        \"https://en.wikipedia.org/wiki/Agentic_AI\"\n",
    "    ]\n",
    ")\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using uv with Jupyter\\n\\nThe Jupyter notebook is a popular tool for interactive computing, data analys'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content[0:100] # testing/checking the page content is correctly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://docs.astral.sh/uv/guides/integration/jupyter/#creating-a-kernel'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata # we'll get the source of the item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting **data into chunks** can be done in `native python` but it is a tidious process. Also if necessary, you may need to experiment with various delimiters in an iterative manner to ensure that:\n",
    "\" each chunk does not exceed the token length limit of the respective LLM. \"\n",
    "\n",
    "`Langchain` provides a better way through text splitter classes.\n",
    "\n",
    "Using **Recursive Text Splitter** Classes from Langchain\n",
    "CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = 200,  # size of each chunk created\n",
    "    chunk_overlap  = 50,  # size of  overlap between chunks in order to maintain the context\n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "195\n",
      "65\n",
      "166\n",
      "95\n",
      "['Using uv with Jupyter', 'The Jupyter notebook is a popular tool for interactive computing, data analysis, and visualization. You can use Jupyter with uv in a few different ways, either to interact with a project, or as a', 'ways, either to interact with a project, or as a standalone tool.', \"Using Jupyter within a project\\n\\nIf you're working within a project, you can start a Jupyter server with access to the project's virtual environment via the following:\", '$ uv run --with jupyter jupyter lab\\n\\nBy default, jupyter lab will start the server at http://lo']\n"
     ]
    }
   ],
   "source": [
    "chunks = splitter.split_text(data[0].page_content[:500])\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))\n",
    "    \n",
    "print(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
