{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK4URbewGotBSfGOIAb1hW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SufyAD/AI-ML/blob/main/agentic_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 Multi-Source RAG Agent\n",
        "\n",
        "A Retrieval-Augmented Generation (RAG) application built using **LangChain's Agent framework** and powered by **multiple retrieval tools**. This agent can automatically route user queries to the most relevant data source using OpenAI's function calling logic.\n",
        "\n",
        "## 💡 What is it?\n",
        "\n",
        "This project implements a **multi-source agentic RAG system** that integrates:\n",
        "- ✅ Web scraping via **WebLoader**\n",
        "- 📚 **Wikipedia** search via API\n",
        "- 🧠 **ArXiv research papers** retrieval tool\n",
        "\n",
        "The core intelligence is driven by the `create_openai_tools_agent` from LangChain, which:\n",
        "- Parses your query\n",
        "- Decides the most appropriate tool (source)\n",
        "- Retrieves the contextual information\n",
        "- Generates an answer using the LLM (OpenAI)\n",
        "\n",
        "No manual routing — everything is handled *agentically* ✨\n",
        "\n",
        "## 🛠️ Tools Used\n",
        "\n",
        "| Source | Description |\n",
        "|--------|-------------|\n",
        "| `WebLoader` | Scrapes content from public web pages |\n",
        "| `WikipediaSearchTool` | Performs semantic search over Wikipedia topics |\n",
        "| `ArxivTool` | Retrieves summaries of research papers from Arxiv.org |\n",
        "\n",
        "## 📦 Tech Stack\n",
        "\n",
        "- LangChain (Agents, Tools, RAG)\n",
        "- Gemini (GPT-4)\n",
        "- Python\n",
        "- Arxiv API / Wikipedia API\n",
        "- VectorStore (e.g., FAISS or Chroma)\n",
        "\n",
        "## 🧠 Agentic RAG in Action\n",
        "\n",
        "Check out this diagram for a peek behind the scenes:  \n",
        "*(Attach your chosen diagram here)*"
      ],
      "metadata": {
        "id": "EHJzH55nhHFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### requirements\n",
        "\n",
        "*   wikipedia\n",
        "*   arxiv\n",
        "*   langchain_community\n",
        "*   langchain_google_genai\n",
        "*   faiss-cpu\n",
        "\n"
      ],
      "metadata": {
        "id": "z-cTiJdc42x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community langchain_text_splitters langchain_google_genai langchain_huggingface langchain_core langchainhub langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m_wOZNRkokY",
        "outputId": "c106beff-4f0a-4b4a-d3b5-242952b42645"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d-gI25HJ3Jiz"
      },
      "outputs": [],
      "source": [
        "!pip install -qU wikipedia arxiv faiss-cpu sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool 1: Create `Wikipedia Search Tool`"
      ],
      "metadata": {
        "id": "PU6PjdFdgoJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool 1\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "\n",
        "api_wrapper = WikipediaAPIWrapper(top_k_results=1, lang='en', max_chars=200)\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "wiki_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D0lKLoqo5Yh8",
        "outputId": "6658af5f-833d-4c4d-b3c1-d067d237d98c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipedia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool 2: Create `WebBase Loader`"
      ],
      "metadata": {
        "id": "vZqbpmfj5bZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool 2\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# These are the steps to store embedded-data in the vector db\n",
        "#Step1: data ingestion\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
        "data = loader.load()\n",
        "\n",
        "# Step2: Character text splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(data) # text splitted docs\n",
        "\n",
        "# Step3: Store in Vector DB, FAISS in our case\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "db = FAISS.from_documents(docs[:100], embeddings)\n",
        "\n",
        "# Step4: Create retriever from vector DB\n",
        "web_tool = db.as_retriever()"
      ],
      "metadata": {
        "id": "HwLZOP3i6aXZ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note .invoke == document_get_similar in the new API\n",
        "web_tool.invoke(\"What is Extended Neural GPU?\")"
      ],
      "metadata": {
        "id": "Wr_mEusnAMW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating WebTool Wrapper as existing tool is not compatible with OpenAI function calling format\n",
        "# from langchain.agents import Tool\n",
        "# def run_web_tool(query: str) -> str:\n",
        "#     \"\"\"Searches the vector database for relevant information.\"\"\"\n",
        "#     return web_tool.get_relevant_documents(query)[0].page_content # getting retrieved response from vectordb\n",
        "\n",
        "# web_tool_langchain = Tool(\n",
        "#     name = \"web_base_search\",\n",
        "#     func=run_web_tool,\n",
        "#     description=\"Useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
        "# )\n",
        "# web_tool_langchain\n",
        "\n",
        "\n",
        "###----- Alternatively, we can use retrieval tool from langchain\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "web_tool_langchain = create_retriever_tool(\n",
        "    retriever=web_tool,\n",
        "    name=\"web_base_search\",\n",
        "    description=\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\"\n",
        ")\n",
        "web_tool_langchain.invoke(\"LangSmith is framework-agnostic?\")"
      ],
      "metadata": {
        "id": "sT1WVMGk9_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool 3: Create `Arxiv Research-paper Searcher`"
      ],
      "metadata": {
        "id": "aW1ojd3Ugsx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool 3\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "from langchain_community.tools import ArxivQueryRun\n",
        "\n",
        "arxiv_api_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
        "arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_api_wrapper)\n",
        "arxiv_tool.name"
      ],
      "metadata": {
        "id": "7YJL1NkPczJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create `OpenAI Tool-Agent` including the LLM(binded with tools)\n",
        "*   I am using `create_openai_tools_agent` to create agent\n",
        "*   To run this agent we need `agent_executor` that'll be able to understand the context and pass the query to respective tool\n",
        "\n"
      ],
      "metadata": {
        "id": "tRUxm15Eiua3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of tools to be used the LLM\n",
        "tools = [wiki_tool, web_tool_langchain, arxiv_tool]\n",
        "tools"
      ],
      "metadata": {
        "id": "pyp7UNwWiwCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ----------- Calling predefined prompts in the Langchain Hub\n",
        "from langchain import hub\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "prompt.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOzFghmqkVrt",
        "outputId": "cad1a6e2-3010-4419-94dc-2e1547dac421"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='agent_scratchpad')]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(google_api_key=userdata.get('GEMINI_API_KEY'), model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "5TmJe_l4e2k4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_openai_tools_agent, Tool, AgentExecutor\n",
        "from langchain.agents import AgentExecutor\n",
        "# 1. Create the agent\n",
        "agent = create_openai_tools_agent(llm=gemini, tools=tools, prompt=prompt)\n",
        "agent"
      ],
      "metadata": {
        "id": "EOtAs5gPjZ0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create agent_executor, This is our final agent_executor that will use the tools depending on the context\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "Q587CruumBqE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"LangSmith allows me to closely monitor my applications? Is this right?\"\n",
        "})"
      ],
      "metadata": {
        "id": "N2qtbW_Y_N7Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}